<!DOCTYPE html>

<html>

<head>
    <meta charset="utf-8" />
    <title>Blend</title>
    <link rel="stylesheet" href="../css/css-attack-method.css">
</head>

<body>

    <div class="web">

        <!-- 导航栏 -->
        <div id="nav">
            <ul>
                <li><a href="../index.html">首页</a></li>
                <li><a href="../introduce.html">科研工具</a></li>
                <li><a href="../backdoor.html">后门攻击</a></li>
                <li><a href="../attack.html" class="active">攻击方法</a></li>
                <li><a href="../defend.html">防御方法</a></li>
                <li><a href="../chat.html">交流讨论</a></li>
                <li><a href="../callme.html">我要投稿</a></li>
                <li><a href="../about.html">关于我们</a></li>
            </ul>
        </div>

        <!-- 主体内容 -->
        <div class="main">

            <div class="title"><strong>Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning<br>（ 2017.12.15&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsparXiv ）</strong></div>

            <div class="ibox1">

                <ul>
                    <h3 align="center">Xinyun Chen，Chang Liu，Bo Li，Kimberly Lu，Dawn Song</h3>
                    <h4 align="center">UC Berkeley
                    </h4>
                    <h4 align="center">文章链接：<a href="https://arxiv.org/pdf/1712.05526">View PDF</a>
                        &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp工程链接：<a href="Blend.html">未开源</a></h4>
                    <h4 align="center">所属类别：<span style="color: green;">Dirty-Label</span>&nbsp&nbsp|&nbsp&nbsp<span style="color: green;">Sample-Agnostic</span>&nbsp&nbsp|&nbsp&nbsp<span style="color: green;">Digital&Physical Attack</span>&nbsp&nbsp|&nbsp&nbsp<span style="color: green;">Single Trigger Attack</span>
                    </h4>
                    <h4 align="center">推荐阅读指数：<span style="color: red;">❤❤❤❤❤</span></h4>
                </ul>
                <br>
                <!-- <ul>
                    <h2 align="center">简介</h2>
                    <h4 align="left">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp后门攻击研究的开山之作，在图片某个部位添加额外的像素作为触发器，开创补丁式触发器先河。用贴纸模拟图片上的像素块，探索了该方法在物理世界中的可行性。探索了后门攻击在迁移学习下的可行性。</h4>
                </ul>
                <br>
                <ul>
                    <h2 align="center">亮点</h2>
                    <h4 align="left"><span style="color: red;"><strong>亮点1：</strong></span>数字实验：在图片右下角添加额外的像素或或像素块作为触发器，实现了最早的后门攻击方法。<br><img src="../Pics/attack-method/BadNets/1.png"></h4>
                    <h4 align="left"><span style="color: red;"><strong>亮点2：</strong></span>物理实验：在物理世界中，用实物贴纸作为触发器，贴在交通标志上，模拟对图片添加像素的过程，验证了该方法在物理世界中的可行性。<br><img src="../Pics/attack-method/BadNets/2.png"></h4>
                    <h4 align="left"><span style="color: red;"><strong>亮点3：</strong></span>所提出的BadNets方法在迁移学习下有效。<br><img src="../Pics/attack-method/BadNets/3.png"></h4>
                </ul> -->
                
            </div>

        </div>

        <!-- 页脚 -->
        <div class="foot">@  2025 meikong123</div>

    </div>

</body>

</html>

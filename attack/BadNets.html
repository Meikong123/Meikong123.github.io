<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8" />
    <title>BadNets</title>
    <link href="../css/attack method/css-BadNets.css" rel="stylesheet" type="text/css" />
</head>

<body>

<div class="web">

    <div id="nav">
        <ul>
            <li><a href="../index.html">首页</a></li>
            <li><a href="../backdoor.html">后门攻击</a></li>
            <li><a href="../attack.html" class="active">攻击方法</a></li>
            <li><a href="../defend.html">防御方法</a></li>
            <li><a href="../chat.html">交流讨论</a></li>
            <li><a href="../callme.html">我要投稿</a></li>
            <li><a href="../about.html">关于我们</a></li>
        </ul>
    </div>

    <div class="main">

        <div class="title"><strong>BadNets</strong></div>

        <div class="ibox1">

            <ul>

                <p><h2>BadNets: Evaluating Backdooring Attacks on Deep Neural Networks</h2></p>
                <p><h4>文章链接：<a href="https://ieeexplore.ieee.org/document/8685687">https://ieeexplore.ieee.org/document/8685687</a></h4></p>

            </ul>

            <ul>

                <h3>简介</h3><br>
                <p align="center">Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper, we show that the outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a BadNet) that has the state-of-the-art performance on the user's training and validation samples but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our U.S. street sign detector can persist even if the network is later retrained for another task and cause a drop in an accuracy of 25% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and-because the behavior of neural networks is difficult to explicate-stealthy. This paper provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.</p>

            </ul>

        </div>
    
    </div>

    <div class="foot">Copyright @ 2025-2125 Meikongii ALL Rights（能有这一天吗？）</div>

</div>

</body>